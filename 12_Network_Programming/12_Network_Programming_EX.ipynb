{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the HyperText Transport Protocol\n",
    "\n",
    "You are to retrieve the following document using the HTTP protocol in a way that you can examine the HTTP Response headers.\n",
    "\n",
    "- http://data.pr4e.org/intro-short.txt\n",
    "\n",
    "There are three ways that you might retrieve this web page and look at the response headers:\n",
    "\n",
    "- Preferred: Modify the socket1.py program to retrieve the above URL and print out the headers and data. Make sure to change the code to retrieve the above URL - the values are different for each URL.\n",
    "- Open the URL in a web browser with a developer console or FireBug and manually examine the headers that are returned.\n",
    "- Use the telnet program as shown in lecture to retrieve the headers and content.\n",
    "\n",
    "Enter the header values in each of the fields below and press \"Submit\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"1d3-54f6609240717\"\n",
      "Content-Length: 467\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Content-Type: text/plain\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "text = ''\n",
    "while True:\n",
    "    data = mysock.recv(600)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    for line in data.decode():\n",
    "        text = text + line\n",
    "text = text.splitlines()\n",
    "for i in text:\n",
    "    if re.search('Last-Modified:|ETag:|Content-Length:|Cache-Control:|Content-Type:', i):\n",
    "        print (i)\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Numbers from HTML using BeautifulSoup \n",
    "\n",
    "In this assignment you will write a Python program similar to http://www.py4e.com/code3/urllink2.py. The program will use urllib to read the HTML from the data files below, and parse the data, extracting numbers and compute the sum of the numbers in the file.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "- Sample data: http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553)\n",
    "- Actual data: http://py4e-data.dr-chuck.net/comments_237568.html (Sum ends with 50)\n",
    "\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this, you can install BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# Or download the file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "if url == '':\n",
    "    url = 'http://py4e-data.dr-chuck.net/comments_42.html'\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span')\n",
    "count = 0\n",
    "suma = 0\n",
    "for tag in tags:\n",
    "   # Look at the contents of the 'span' tag\n",
    "    suma = suma + int(tag.contents[0])\n",
    "    count = count + 1\n",
    "print (suma)\n",
    "print (count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following Links in Python\n",
    "\n",
    "In this assignment you will write a Python program that expands on http://www.py4e.com/code3/urllinks.py. The program will use urllib to read the HTML from the data files below, extract the href= vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name in the list, follow that link and repeat the process a number of times and report the last name you find.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the name for your testing and the other is the actual data you need to process for the assignment\n",
    "\n",
    "- Sample problem: Start at http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "    Find the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. The answer is the last name that you retrieve.\n",
    "    Sequence of names: Fikret Montgomery Mhairade Butchi Anayah\n",
    "    Last name in sequence: Anayah\n",
    "    \n",
    "- Actual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Neco.html\n",
    "    Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.\n",
    "    Hint: The first character of the name of the last page that you will load is: O\n",
    "\n",
    "## Strategy\n",
    "\n",
    "The web pages tweak the height between the links and hide the page after a few seconds to make it difficult for you to do the assignment without writing a Python program. But frankly with a little effort and patience you can overcome these attempts to make it a little harder to complete the assignment without writing a Python program. But that is not the point. The point is to write a clever Python program to solve the program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter URL: \n",
      "Enter count: 7\n",
      "Enter position: 18\n",
      "Count number 1\n",
      "Neco 's  18 fiend is: Laughlan \n",
      "\n",
      "Count number 2\n",
      "Laughlan 's  18 fiend is: Arandeep \n",
      "\n",
      "Count number 3\n",
      "Arandeep 's  18 fiend is: Saoirse \n",
      "\n",
      "Count number 4\n",
      "Saoirse 's  18 fiend is: Priscillia \n",
      "\n",
      "Count number 5\n",
      "Priscillia 's  18 fiend is: Caie \n",
      "\n",
      "Count number 6\n",
      "Caie 's  18 fiend is: Tyson \n",
      "\n",
      "Count number 7\n",
      "Tyson 's  18 fiend is: Okeoghene \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter URL: ')\n",
    "count = int(input('Enter count: '))\n",
    "pos = int(input('Enter position: ')) - 1\n",
    "\n",
    "if url == '':\n",
    "    url = 'http://py4e-data.dr-chuck.net/known_by_Neco.html'\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "step = 1\n",
    "while step <= count:\n",
    "    print ('Count number', step)\n",
    "    print (url.split('_')[2].split('.')[0],\"'s \",pos + 1 ,'fiend is:',tags[pos].contents[0], '\\n' )\n",
    "    url = tags[pos].get('href', None)\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags = soup('a')\n",
    "    step = step + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
